{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feea6caf-f163-47df-ac48-b5ba79687a3d",
   "metadata": {},
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "# ANS Min-Max scaling, also known as Min-Max normalization, is a data preprocessing technique used in machine learning and data analysis to transform numerical features into a specific range( between 0 and 1). This scaling method helps ensure that different features with varying scales or units have similar magnitudes, making it easier for machine learning algorithms to converge faster and perform better. It does so by linearly mapping the original feature values to a new range.\n",
    "\n",
    "# Example-Suppose you have a dataset of ages with values ranging from 20 to 40, and you want to scale these ages to a range between 0 and 1 using Min-Max scaling.\n",
    "\n",
    "# Original ages:\n",
    "# Person A: 20 years\n",
    "# Person B: 25 years\n",
    "# Person C: 35 years\n",
    "# Person D: 40 years\n",
    "# To apply Min-Max scaling, you need to find the minimum and maximum values in the dataset:\n",
    "# Xmin  = 20 (minimum age)\n",
    "# Xmax= 40 (maximum age)\n",
    "# Now, you can scale each person's age using the Min-Max scaling formula:\n",
    "# Person A (scaled) = (20−20)/(40−20)=0\n",
    "# Person B (scaled) = (25−20)/(40−20)=0.25\n",
    "# Person C (scaled) = (35−20)/(40−20)=0.75\n",
    "# Person D (scaled) = (40−20)/(40−20)=1.0\n",
    "# After Min-Max scaling, the ages are now in the desired range of 0 to 1, and they have a consistent scale that  can be used for analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342b30b0-9fd4-49a7-8390-c136492fe940",
   "metadata": {},
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? \n",
    "# ANS The Unit Vector technique, also known as vector normalization or feature scaling, is a data preprocessing method used to transform numerical features into unit vectors. Unlike Min-Max scaling, which scales features to a specific range (typically between 0 and 1), unit vector scaling ensures that the magnitude or length of each feature vector is equal to 1. This normalization technique is particularly useful when the direction of the feature vectors matters more than their absolute values.\n",
    "# The primary difference between Min-Max scaling and unit vector scaling is that Min-Max scaling scales features to a predefined range while preserving the original direction of the data, whereas unit vector scaling focuses solely on the direction of the feature vectors and sets their length to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a554ba15-d86a-40ce-be7b-0a1a9f8befbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "# ANS PCA, which stands for Principal Component Analysis, is a widely used technique in the field of machine learning and data analysis for dimensionality reduction and feature extraction. Its primary purpose is to reduce the dimensionality of a dataset while preserving as much of the variance (information) in the data as possible. PCA achieves this by transforming the original features into a new set of orthogonal (uncorrelated) features called principal components.\n",
    "# PCA working:\n",
    "1. **Data Centering**: First, PCA centers the data by subtracting the mean of each feature from the data points. This ensures that the data is centered around the origin.\n",
    "2. **Covariance Matrix**: PCA then calculates the covariance matrix of the centered data. The covariance matrix provides information about the relationships between features and their variances.\n",
    "3. **Eigenvalue Decomposition**: Next, PCA performs eigenvalue decomposition on the covariance matrix. This decomposition yields a set of eigenvalues and corresponding eigenvectors.\n",
    "4. **Selecting Principal Components**: The eigenvectors represent the directions (principal components) along which the data varies the most. PCA sorts the eigenvectors by their corresponding eigenvalues in descending order. The eigenvector with the highest eigenvalue represents the principal component with the most variance and so on.\n",
    "5. **Dimensionality Reduction**: To reduce the dimensionality of the data, you can select a subset of the top-k principal components, where k is less than the original number of features. These selected principal components capture most of the variance in the data.\n",
    "6. **Projection**: Finally, you can project the original data onto the selected principal components to obtain a lower-dimensional representation of the data.\n",
    "\n",
    "\n",
    "# Suppose you have a dataset with two features: the length and width of various rectangles. You want to reduce the dimensionality of this dataset using PCA.\n",
    "\n",
    "- Original dataset (rectangles):\n",
    "  - Rectangle A: Length = 5 units, Width = 3 units\n",
    "  - Rectangle B: Length = 4 units, Width = 2 units\n",
    "  - Rectangle C: Length = 7 units, Width = 4 units\n",
    "\n",
    "1. **Data Centering**: Calculate the mean of the length and width. Let's say the mean length is 5.33 units, and the mean width is 3 units. Subtract these values from each rectangle's length and width.\n",
    "2. **Covariance Matrix**: Calculate the covariance matrix based on the centered data.\n",
    "3. **Eigenvalue Decomposition**: Perform eigenvalue decomposition to find the eigenvectors and eigenvalues.\n",
    "4. **Selecting Principal Components**: Suppose you find two eigenvectors with corresponding eigenvalues of 4.2 and 0.3. The first principal component (eigenvector with the highest eigenvalue) captures the primary variation in the data, while the second principal component captures less variation.\n",
    "5. **Dimensionality Reduction**: Choose to keep the first principal component, reducing the dimensionality from 2 to 1.\n",
    "6. **Projection**: Project the original data onto the first principal component to obtain a lower-dimensional representation. The new representation of the rectangles might look like this:\n",
    "\n",
    "   - Rectangle A: PC1 = 2.0 units\n",
    "   - Rectangle B: PC1 = 1.4 units\n",
    "   - Rectangle C: PC1 = 3.6 units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc463dd-c657-442f-b9bb-7e24112a8799",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "# ANS PCA (Principal Component Analysis) is closely related to feature extraction in the context of dimensionality reduction. Feature extraction is a broader concept that encompasses techniques for reducing the dimensionality of data by transforming or extracting new features from the original features. PCA is one such technique used for feature extraction.\n",
    "# The relationship between PCA and feature extraction can be summarized as follows:\n",
    "- Dimensionality Reduction: Both PCA and feature extraction aim to reduce the dimensionality of a dataset. High-dimensional data can be challenging to work with and may suffer from the curse of dimensionality, leading to increased computational complexity and potential overfitting in machine learning models.\n",
    "- Transformation: PCA transforms the original features into a set of new features called principal components. These components are linear combinations of the original features and are ordered by the amount of variance they capture. Feature extraction techniques, in general, involve transforming the original features into a new set of features that capture relevant information or patterns in the data.\n",
    "- Preservation of Information: PCA seeks to preserve as much of the variance in the data as possible when reducing dimensionality. It achieves this by selecting the top-k principal components, which collectively capture most of the variance. Other feature extraction methods may focus on preserving different aspects of the data's information, such as discriminative power or interpretability.\n",
    "# EXAMPLE\n",
    "# Original dataset (exam scores)\n",
    "# Student A: Math = 90, Science = 85, English = 88\n",
    "# Student B: Math = 78, Science = 92, English = 80\n",
    "# Student C: Math = 85, Science = 88, English = 75\n",
    "# Data Preparation: You have a dataset with three original features: Math, Science, and English scores for each # student.\n",
    "\n",
    "# PCA: Apply PCA to the dataset. PCA will calculate the covariance matrix of the three scores, find the principal components (eigenvectors), and sort them by the amount of variance they capture (eigenvalues).\n",
    "\n",
    "- Selecting Principal Components: Choose to retain only one principal component to simplify the data while still capturing most of the variance. You decide to keep just the top principal component.\n",
    "- Feature Extraction: Project the original data onto the selected principal component. This projection results in a new feature for each student, which is a linear combination of the original Math, Science, and English scores.\n",
    "\n",
    "# The resulting feature is a weighted combination of the three original scores, emphasizing the direction of maximum variance in the data. This single feature extracted by PCA can be used for further analysis or modeling, reducing the dimensionality of the dataset from three features to one.\n",
    "\n",
    "# For example, the new feature values for each student might look like this:\n",
    "\n",
    "# Student A (PCA feature) ≈ 0.739\n",
    "# Student B (PCA feature) ≈ 2.481\n",
    "# Student C (PCA feature) ≈ -1.220\n",
    "# In this simplified example, PCA has reduced the dimensionality of the original data while retaining the most critical information in  a single feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df0b3639-323e-48cc-8e99-ad10e30e46f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prices: [10 20 30 15 25]\n",
      "Scaled Prices: [0.   0.5  1.   0.25 0.75]\n",
      "Original Ratings: [3.5 4.2 3.  4.8 3.9]\n",
      "Scaled Ratings: [0.27777778 0.66666667 0.         1.         0.5       ]\n",
      "Original Delivery Times: [40 50 35 45 55]\n",
      "Scaled Delivery Times: [0.25 0.75 0.   0.5  1.  ]\n"
     ]
    }
   ],
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "# contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "# preprocess the data.\n",
    "import numpy as np\n",
    "\n",
    "prices = np.array([10, 20, 30, 15, 25])\n",
    "ratings = np.array([3.5, 4.2, 3.0, 4.8, 3.9])\n",
    "delivery_times = np.array([40, 50, 35, 45, 55])\n",
    "\n",
    "def min_max_scaling(feature):\n",
    "    min_val = np.min(feature)\n",
    "    max_val = np.max(feature)\n",
    "    scaled_feature = (feature - min_val) / (max_val - min_val)\n",
    "    return scaled_feature\n",
    "\n",
    "scaled_prices = min_max_scaling(prices)\n",
    "scaled_ratings = min_max_scaling(ratings)\n",
    "scaled_delivery_times = min_max_scaling(delivery_times)\n",
    "\n",
    "print(\"Original Prices:\", prices)\n",
    "print(\"Scaled Prices:\", scaled_prices)\n",
    "\n",
    "print(\"Original Ratings:\", ratings)\n",
    "print(\"Scaled Ratings:\", scaled_ratings)\n",
    "\n",
    "print(\"Original Delivery Times:\", delivery_times)\n",
    "print(\"Scaled Delivery Times:\", scaled_delivery_times)\n",
    "\n",
    "# After applying Min-Max scaling to these features, your dataset will have these features transformed \n",
    "#into a common range, ensuring that no single feature dominates the others in terms of scale.\n",
    "#This helps in building a recommendation system that can provide balanced and meaningful recommendations by \n",
    "#considering these features equally. The scaled features can then be used as inputs to various recommendation algorithms,\n",
    "#such as collaborative filtering or content-based filtering, to make personalized food recommendations \n",
    "#to users based on their preferences and constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369bddc-e71a-4455-8e1a-fbd3fa1c1872",
   "metadata": {},
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "# ANS  Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset in a stock price prediction project can be beneficial, especially when dealing with a large number of features. Here's a step-by-step explanation of how you would use PCA for dimensionality reduction in this context:\n",
    "\n",
    "- Data Preparation:Gather your dataset, which includes various features related to company financial data and market trends. These features may include things like earnings per share, market capitalization, historical stock prices, trading volumes, economic indicators, and more.\n",
    "- Feature Scaling:Standardize or normalize your features. This step is crucial to ensure that features with different units and scales are treated equally during PCA. You can use techniques like Min-Max scaling or z-score standardization to make sure all features have a similar scale.\n",
    "- PCA Application:Apply PCA to your standardized or normalized dataset. PCA will transform your original features into a new set of uncorrelated features called principal components. These principal components are linear combinations of the original features and capture the maximum variance in the data.\n",
    "- Choosing the Number of Principal Components:One critical decision is determining how many principal components to retain. This can be done by examining the explained variance ratio. You typically want to retain enough principal components to explain a significant portion of the total variance (e.g., 95% or 99%).\n",
    "- Model Building:Use the reduced-dimension dataset (X_reduced) for training your stock price prediction model. You may use various machine learning algorithms like regression, time series forecasting, or neural networks.\n",
    "- Evaluation and Tuning:Evaluate the performance of your model using appropriate metrics, such as Mean Absolute Error (MAE) or Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5420e01d-6a59-423e-ba16-d63a4092e13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Values: [1, 5, 10, 15, 20]\n",
      "Scaled Values: [0.0, 0.21052631578947367, 0.47368421052631576, 0.7368421052631579, 1.0]\n",
      "Final Values : [-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "# values to a range of -1 to 1.\n",
    "\n",
    "original_values = [1, 5, 10, 15, 20]\n",
    "X_min = min(original_values)\n",
    "X_max = max(original_values)\n",
    "scaled_values = [(x - X_min) / (X_max - X_min) for x in original_values]\n",
    "\n",
    "final_values = [2 * x - 1 for x in scaled_values]\n",
    "\n",
    "print(\"Original Values:\", original_values)\n",
    "print(\"Scaled Values:\", scaled_values)\n",
    "print(\"Final Values :\", final_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cfbbbb0-300e-4ae3-b1cf-0e709d5484d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m     25\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)  \n\u001b[0;32m---> 26\u001b[0m pca\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX\u001b[49m)\n\u001b[1;32m     28\u001b[0m explained_variance_ratio \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mexplained_variance_ratio_\n\u001b[1;32m     29\u001b[0m cumulative_explained_variance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcumsum(explained_variance_ratio)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "# Feature Extraction using PCA. How many principal components would you choose to retain, and why\n",
    "# When performing feature extraction using PCA (Principal Component Analysis), one of the crucial decisions is determining how many \n",
    "# principal components to retain. The number of principal components to keep depends on your specific goals and the amount of variance \n",
    " #you want to preserve in the data. Here's a general approach to decide how many principal components to retain:\n",
    "\n",
    "# 1.Calculate Explained Variance: After applying PCA to your dataset, you will obtain a set of principal components, each associated\n",
    "# with an explained variance ratio. The explained variance ratiofor each principal component indicates the proportion of the \n",
    "# total variance in the data that is captured by that component.\n",
    "\n",
    "# 2.Cumulative Explained Variance:- Calculate the cumulative explained variance by summing the explained variance ratios from the \n",
    "# first principal component to the kth principal component.\n",
    "# - You can use this cumulative explained variance to determine how much of the total variance is retained as you increase the\n",
    "# number of retained components.\n",
    "\n",
    "#3.Threshold for Retention: Choose a threshold for the amount of variance you want to retain. This threshold can be based on the\n",
    "# desired level of information preservation.\n",
    "# - Common thresholds are 95% or 99% of the total variance, but you can adjust this based on your specific needs.\n",
    "\n",
    "#4.Decision on the Number of Components: Decide how many principal components to retain based on the cumulative explained variance\n",
    "# and your chosen threshold. You typically retain enough components to exceed or closely approach the chosen threshold.\n",
    " # - Fewer components retain less variance but result in dimensionality reduction, while more components retain more variance but may retain noise.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Hypothetical dataset with features [height, weight, age, gender, blood pressure]\n",
    "# Replace this with your actual dataset\n",
    "X = np.array([\n",
    "    [170, 65, 30, 0, 120],\n",
    "    [162, 55, 28, 1, 118],\n",
    "    [175, 70, 35, 1, 125],\n",
    "    # ... Add more data points\n",
    "])\n",
    "\n",
    "# Standardize the dataset (important before applying PCA)\n",
    "mean = np.mean(X, axis=0)\n",
    "std_dev = np.std(X, axis=0)\n",
    "X_standardized = (X - mean) / std_dev\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=None)  # Retain all components\n",
    "pca.fit(X_standardized)\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Choose a threshold (e.g., 95% of variance)\n",
    "threshold = 0.95\n",
    "\n",
    "# Find the number of components\n",
    "num_components_to_retain = np.argmax(cumulative_explained_variance >= threshold) + 1\n",
    "\n",
    "print(\"Explained Variance Ratios:\", explained_variance_ratio)\n",
    "print(\"Cumulative Explained Variance:\", cumulative_explained_variance)\n",
    "print(\"Number of Components to Retain:\", num_components_to_retain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511ad76-e209-4134-b063-f0c51803a2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
